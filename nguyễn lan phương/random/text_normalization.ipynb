{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "NGHỊ ĐỊNH\n",
    "\n",
    "Sửa đổi, bổ sung một số điều của các Nghị định liên quan đến quản lý hoạt động vận tải bằng xe ô tô, dịch vụ đào tạo lái xe ô tô và dịch vụ sát hạch lái xe\n",
    "\n",
    "5. Sửa đổi, bổ sung Điều 11 (được sửa đổi tại điểm a khoản 1 Điều 2 Nghị định số 70/2022/NĐ-CP ngày 27 tháng 9 năm 2022) như sau:\n",
    "\n",
    "\"Điều 11. Giấy phép xe tập lái và thẩm quyền cấp Giấy phép xe tập lái\n",
    "\n",
    "1. Sở Giao thông vận tải cấp Giấy phép xe tập lái cho xe tập lái của cơ sở đào tạo do địa phương quản lý theo mẫu quy định tại Mẫu số 01, Mẫu số 02 Phụ lục VII kèm theo Nghị định này; có hiệu lực trong ứng với thời gian được phép lưu hành ghi trên Giấy Chứng nhận kiểm định an toàn kỹ thuật và bảo vệ môi trường xe cơ giới do đơn vị đăng kiểm thuộc Bộ Giao thông vận tải cấp đối với xe tập lái.\n",
    "\n",
    "2. Giấy phép xe tập lái bị thu hồi trong các trường hợp sau:\n",
    "\n",
    "a) Các cơ sở đào tạo lái không đáp ứng một trong điều kiện quy định tại khoản 2 Điều 3 và khoản 2 Điều 6 Nghị định này;\n",
    "\n",
    "b) Bị tẩy xóa, sửa chữa;\n",
    "\n",
    "c) Cơ sở đào tạo lái xe không còn nhu cầu sử dụng xe tập lái;\n",
    "\n",
    "d) Không bảo đảm các yêu cầu an toàn kỹ thuật và bảo vệ môi trường;\n",
    "\n",
    "đ) Giấy phép xe tập lái đã được cấp lại, đổi, thay thế;\n",
    "\n",
    "e) Xe tập lái đã hết thời hạn sử dụng theo quy định.\n",
    "\n",
    "3. Cơ quan cấp thẩm quyền cấp Giấy phép xe tập lái, thu hồi, cấp lại, đổi, thay thế Giấy phép xe tập lái trong các trường hợp sau:\n",
    "\n",
    "a) Thu hồi Giấy phép xe tập lái đối với các trường hợp quy định tại khoản 2 Điều này;\n",
    "\n",
    "b) Cấp lại Giấy phép xe tập lái đối với các trường hợp Giấy phép xe tập lái bị mất, hư hỏng;\n",
    "\n",
    "c) Đổi Giấy phép xe tập lái khi Giấy phép xe tập lái bị tẩy xóa, sửa chữa hoặc có nhu cầu đổi Giấy phép xe tập lái của cơ sở đào tạo lái xe;\n",
    "\n",
    "d) Thay thế Giấy phép xe tập lái đã hết hạn sử dụng xe tập lái bằng Giấy phép xe tập lái mới.\n",
    "\n",
    "Trường hợp các cơ quan đã tiếp nhận hồ sơ thu hồi Giấy phép xe tập lái của cơ sở đào tạo lái xe, đơn vị đăng kiểm sẽ ngừng việc kiểm định đối với các xe tập lái của cơ sở đào tạo lái xe đã bị thu hồi Giấy phép xe tập lái theo quy định.\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "texts = text_splitter.split_text(text)\n",
    "\n",
    "texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=\"sk-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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "# Initialize the OpenAI language model\n",
    "your_language_model = OpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"], model_name=\"gpt-4o-mini\")  # or any other model you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Given the following text chunk, identify the main topic and detect if there is a topic change compared to the previous chunk.\n",
    "\n",
    "Text Chunk: {text_chunk}\n",
    "\n",
    "Main Topic: {main_topic}\n",
    "Topic Change: {topic_change}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text_chunk\", \"main_topic\", \"topic_change\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=your_language_model)\n",
    "topic_changes = []\n",
    "\n",
    "for i, text_chunk in enumerate(texts):\n",
    "    if i == 0:\n",
    "        previous_topic = \"None\"\n",
    "    else:\n",
    "        previous_topic = topic_changes[-1][\"main_topic\"]\n",
    "\n",
    "    response = llm_chain.run({\n",
    "        \"text_chunk\": text_chunk,\n",
    "        \"main_topic\": previous_topic,\n",
    "        \"topic_change\": \"No\"\n",
    "    })\n",
    "    topic_changes.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the nltk suite\n",
    "import nltk\n",
    "\n",
    "# importing edit distance\n",
    "from nltk.metrics.distance  import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and importing package 'words'\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "correct_words = words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "wrong_word = \"nhappy\"\n",
    "\n",
    "edit_distances = []\n",
    "\n",
    "for correct_word in correct_words:\n",
    "    if len(wrong_word) - 2 <= len(correct_word) and  len(correct_word) <= len(wrong_word) + 2 :\n",
    "        e = edit_distance(wrong_word, correct_word)\n",
    "        edit_distances.append((correct_word, e))\n",
    "\n",
    "result = sorted(edit_distances, key = lambda item:item[1])\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "# Print the results and timing\n",
    "print(f\"Time taken: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_distances[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that need to be corrected\n",
    "incorrect_words=['happpy', 'azmaing', 'intelliengt']\n",
    "\n",
    "# loop for finding correct spellings\n",
    "# based on edit distance and\n",
    "# printing the correct words\n",
    "for word in incorrect_words:\n",
    "    temp = [(edit_distance(word, w),w) for w in correct_words if w[0]==word[0]]\n",
    "    print('sorted(temp, key = lambda val:val[0])[0]', sorted(temp, key = lambda val:val[0])[0])\n",
    "    print(sorted(temp, key = lambda val:val[0])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# list of incorrect spellings\n",
    "# that need to be corrected\n",
    "incorrect_words=['nứg', 'azmaing', 'intelliengt']\n",
    "\n",
    "# loop for finding correct spellings\n",
    "# based on jaccard distance\n",
    "# and printing the correct word\n",
    "for word in incorrect_words:\n",
    "    temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                              set(ngrams(w, 2))),w)\n",
    "            for w in correct_words if w[0]==word[0]]\n",
    "    print(sorted(temp, key = lambda val:val[0])[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-speed lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://storage.googleapis.com/protonx-cloud-storage/datasets/IMDB%20Dataset.csv\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"IMDB Dataset.csv\", \"wb\") as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # lemmatize and remove stop words\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_review'] = df['review'].apply(process_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['numeric_sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(example):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(example['review'])\n",
    "    # Lemmatize and remove stop words\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.lower() not in stop_words]\n",
    "    # Re-join tokens into a string\n",
    "    example['text'] = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    # Convert the label 'positive' -> 1, 'negative' -> 0\n",
    "    example['label'] = 1 if example['sentiment'] == 'positive' else 0\n",
    "\n",
    "    return example\n",
    "\n",
    "# Load the IMDB dataset CSV using Hugging Face's load_dataset function\n",
    "dataset = load_dataset('csv', data_files='/content/IMDB Dataset.csv')\n",
    "\n",
    "# Apply the preprocessing function using map\n",
    "processed_dataset = dataset['train'].map(preprocess_text)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "processed_dataset = processed_dataset.remove_columns(['review', 'sentiment'])\n",
    "\n",
    "# Display a sample of the processed dataset\n",
    "print(processed_dataset)\n",
    "\n",
    "# Save the processed Hugging Face Dataset to disk if needed\n",
    "processed_dataset.save_to_disk('processed_imdb_hf_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(example):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(example['review'])\n",
    "    # Lemmatize and remove stop words\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.lower() not in stop_words]\n",
    "    # Re-join tokens into a string\n",
    "    example['text'] = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    # Convert the label 'positive' -> 1, 'negative' -> 0\n",
    "    example['label'] = 1 if example['sentiment'] == 'positive' else 0\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files='IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset['train'].map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f23653f37046f0866c13752a23f097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove unnecessary columns\n",
    "processed_dataset = processed_dataset.remove_columns(['review', 'sentiment'])\n",
    "\n",
    "# Display a sample of the processed dataset\n",
    "print(processed_dataset)\n",
    "\n",
    "processed_dataset.save_to_disk('processed_imdb_hf_dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
