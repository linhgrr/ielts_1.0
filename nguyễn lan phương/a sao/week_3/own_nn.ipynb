{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's together build a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:50:52.141357Z",
     "start_time": "2025-02-21T15:50:51.891906Z"
    }
   },
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function is to initialize param matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:58:16.532488Z",
     "start_time": "2025-02-21T15:58:16.524077Z"
    }
   },
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The neural network we build has 2 module: forward module and backward module"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Linear forward:\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter\n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:51:00.356702Z",
     "start_time": "2025-02-21T15:51:00.351937Z"
    }
   },
   "source": [
    "def linear_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:51:01.926832Z",
     "start_time": "2025-02-21T15:51:01.916802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.array([[-1.02387576, 1.12397796],\n",
    " [-1.62328545, 0.64667545],\n",
    " [-1.74314104, -0.59664964]])\n",
    "W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])\n",
    "b = np.array([[1]])"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:51:03.585130Z",
     "start_time": "2025-02-21T15:51:03.577753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Z, linear_cache = linear_forward(X, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[-0.8019545   3.85763489]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Linear-activation forward:\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value\n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:51:06.424693Z",
     "start_time": "2025-02-21T15:51:06.419533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def relu(Z):\n",
    "    activation_cache = Z\n",
    "\n",
    "    return np.maximum(0, Z), activation_cache"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:51:07.337307Z",
     "start_time": "2025-02-21T15:51:07.332110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(Z):\n",
    "    activation_cache = Z\n",
    "\n",
    "    return 1/(1 + np.exp(-Z)), activation_cache"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:51:08.696715Z",
     "start_time": "2025-02-21T15:51:08.689690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Implement the softmax activation function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of shape (number of classes, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    A -- output of softmax(Z), same shape as Z\n",
    "    cache -- returns Z as well, which will be useful during backpropagation\n",
    "    \"\"\"\n",
    "    expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    A = expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single softmax unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    A, _ = softmax(Z)\n",
    "    dZ = dA * A * (1 - A)\n",
    "    return dZ"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:51:12.408110Z",
     "start_time": "2025-02-21T15:51:12.399263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well.\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:55:33.005368Z",
     "start_time": "2025-02-21T15:55:32.996329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    if activation == \"softmax\":\n",
    "        A, activation_cache = softmax(Z)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)  # (A, W, b, Z)\n",
    "\n",
    "    return A, cache"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:52:06.545108Z",
     "start_time": "2025-02-21T15:52:06.539573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def L_model_forward(X, parameters):\n",
    "\n",
    "    L = len(parameters) // 2\n",
    "    caches = []\n",
    "    A = X\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, linear_activation_cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation=\"softmax\")\n",
    "    caches.append(linear_activation_cache)\n",
    "\n",
    "    return AL, caches"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cost function"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:55:37.067973Z",
     "start_time": "2025-02-21T15:55:37.059736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1] # number of sample\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        cost -= ( Y[0][i] * np.log(AL[0][i]) + (1 - Y[0][i]) * np.log( 1 - AL[0][i] ) )\n",
    "    cost /= m\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    return cost"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Backward module"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    # Here cache is \"linear_cache\" containing (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T15:57:22.621108Z",
     "start_time": "2025-02-21T15:57:22.613200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l\n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:16:28.938494Z",
     "start_time": "2025-02-21T16:16:28.928857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    global dZ\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return dA_prev, dW, db"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SOFTMAX group\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"softmax\" (it's caches[L-1])\n",
    "\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ...\n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ...\n",
    "    \"\"\""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:04:33.117717Z",
     "start_time": "2025-02-21T16:04:33.076725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1] # Last Layer\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"softmax\")\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:04:50.624342Z",
     "start_time": "2025-02-21T16:04:50.613597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "                  parameters[\"W\" + str(l)] = ...\n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:11:08.401532Z",
     "start_time": "2025-02-21T16:11:08.394343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:12:37.448249Z",
     "start_time": "2025-02-21T16:12:26.831600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and preprocess the dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X, y = mnist['data'], mnist['target']\n",
    "X = X.T / 255.0\n",
    "y = np.array(y).astype(int).reshape(1, -1)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:15:17.896757Z",
     "start_time": "2025-02-21T16:15:15.611657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "Y = encoder.fit_transform(y.T).T\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X.T, Y.T, test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y_train, Y_test = X_train.T, X_test.T, Y_train.T, Y_test.T"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:15:19.456856Z",
     "start_time": "2025-02-21T16:15:19.442364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer_dims = [784, 128, 64, 10]\n",
    "# Initialize parameters\n",
    "parameters = initialize_parameters_deep(layer_dims)\n",
    "\n",
    "# Training the model\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T16:19:25.061283Z",
     "start_time": "2025-02-21T16:16:35.781963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation\n",
    "    AL, caches = L_model_forward(X_train, parameters)\n",
    "\n",
    "    # Compute cost\n",
    "    cost = compute_cost(AL, Y_train)\n",
    "\n",
    "    # Backward propagation\n",
    "    grads = L_model_backward(AL, Y_train, caches)\n",
    "\n",
    "    # Update parameters\n",
    "    parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Cost after epoch {epoch}: {cost}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.693091556102413\n",
      "Cost after epoch 100: 0.639791261654793\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m cost \u001B[38;5;241m=\u001B[39m compute_cost(AL, Y_train)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Backward propagation\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m grads \u001B[38;5;241m=\u001B[39m \u001B[43mL_model_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mAL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaches\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Update parameters\u001B[39;00m\n\u001B[0;32m     12\u001B[0m parameters \u001B[38;5;241m=\u001B[39m update_parameters(parameters, grads, learning_rate)\n",
      "Cell \u001B[1;32mIn[21], line 24\u001B[0m, in \u001B[0;36mL_model_backward\u001B[1;34m(AL, Y, caches)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mrange\u001B[39m(L\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)):\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m# lth layer: (RELU -> LINEAR) gradients.\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m# Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\u001B[39;00m\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;66;03m### START CODE HERE ### (approx. 5 lines)\u001B[39;00m\n\u001B[0;32m     23\u001B[0m     current_cache \u001B[38;5;241m=\u001B[39m caches[l]\n\u001B[1;32m---> 24\u001B[0m     dA_prev_temp, dW_temp, db_temp \u001B[38;5;241m=\u001B[39m \u001B[43mlinear_activation_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdA\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43ml\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcurrent_cache\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrelu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m     grads[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdA\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(l)] \u001B[38;5;241m=\u001B[39m dA_prev_temp\n\u001B[0;32m     26\u001B[0m     grads[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdW\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(l \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)] \u001B[38;5;241m=\u001B[39m dW_temp\n",
      "Cell \u001B[1;32mIn[37], line 19\u001B[0m, in \u001B[0;36mlinear_activation_backward\u001B[1;34m(dA, cache, activation)\u001B[0m\n\u001B[0;32m     16\u001B[0m     dZ \u001B[38;5;241m=\u001B[39m softmax_backward(dA, activation_cache)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m### START CODE HERE ### (≈ 1 line of code)\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m dA_prev, dW, db \u001B[38;5;241m=\u001B[39m \u001B[43mlinear_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdZ\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlinear_cache\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m### END CODE HERE ###\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dA_prev, dW, db\n",
      "Cell \u001B[1;32mIn[18], line 5\u001B[0m, in \u001B[0;36mlinear_backward\u001B[1;34m(dZ, cache)\u001B[0m\n\u001B[0;32m      2\u001B[0m A_prev, W, b \u001B[38;5;241m=\u001B[39m cache\n\u001B[0;32m      3\u001B[0m m \u001B[38;5;241m=\u001B[39m A_prev\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m----> 5\u001B[0m dW \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdZ\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mA_prev\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m/\u001B[39mm\n\u001B[0;32m      6\u001B[0m db \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(dZ, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m/\u001B[39mm\n\u001B[0;32m      7\u001B[0m dA_prev \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(W\u001B[38;5;241m.\u001B[39mT, dZ)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
